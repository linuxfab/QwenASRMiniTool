"""
asr_utils.py â€” å…±ç”¨å·¥å…·å‡½å¼èˆ‡å¸¸æ•¸

é›†ä¸­ app.pyã€app-gpu.pyã€chatllm_engine.pyã€streamlit_vulkan.py çš„
é‡è¤‡ç¨‹å¼ç¢¼ï¼Œçµ±ä¸€ç¶­è­·ã€‚

åŒ¯å‡ºï¼š
  å¸¸æ•¸  : SAMPLE_RATE, VAD_CHUNK, VAD_THRESHOLD, MAX_GROUP_SEC,
          MAX_CHARS, MIN_SUB_SEC, GAP_SEC, RT_SILENCE_CHUNKS, RT_MAX_BUFFER_CHUNKS
  å‡½å¼  : detect_speech_groups, split_to_lines, srt_ts, assign_ts
  é¡åˆ¥  : RealtimeManager
"""
from __future__ import annotations

import queue
import threading
from typing import TYPE_CHECKING

import numpy as np

if TYPE_CHECKING:
    pass

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# å…±ç”¨å¸¸æ•¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

SAMPLE_RATE          = 16000
VAD_CHUNK            = 512
VAD_THRESHOLD        = 0.5
MAX_GROUP_SEC        = 20
MAX_CHARS            = 20
MIN_SUB_SEC          = 0.6
GAP_SEC              = 0.08

RT_SILENCE_CHUNKS    = 25    # ~0.8s éœéŸ³å¾Œè§¸ç™¼è½‰éŒ„
RT_MAX_BUFFER_CHUNKS = 600   # ~19s ä¸Šé™å¼·åˆ¶è½‰éŒ„


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# VAD åˆ†æ®µ
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def detect_speech_groups(
    audio: np.ndarray,
    vad_sess,
    max_group_sec: int = MAX_GROUP_SEC,
) -> list[tuple[float, float, np.ndarray]]:
    """Silero VAD åˆ†æ®µï¼Œå›å‚³ [(start_s, end_s, chunk), ...]"""
    h  = np.zeros((2, 1, 64), dtype=np.float32)
    c  = np.zeros((2, 1, 64), dtype=np.float32)
    sr = np.array(SAMPLE_RATE, dtype=np.int64)
    n  = len(audio) // VAD_CHUNK
    probs = []
    for i in range(n):
        chunk = audio[i*VAD_CHUNK:(i+1)*VAD_CHUNK].astype(np.float32)[np.newaxis, :]
        out, h, c = vad_sess.run(None, {"input": chunk, "h": h, "c": c, "sr": sr})
        probs.append(float(out[0, 0]))
    if not probs:
        return [(0.0, len(audio) / SAMPLE_RATE, audio)]

    MIN_CH = 16; PAD = 5; MERGE = 16
    raw: list[tuple[int, int]] = []
    in_sp = False; s0 = 0
    for i, p in enumerate(probs):
        if p >= VAD_THRESHOLD and not in_sp:
            s0 = i; in_sp = True
        elif p < VAD_THRESHOLD and in_sp:
            if i - s0 >= MIN_CH:
                raw.append((max(0, s0-PAD), min(n, i+PAD)))
            in_sp = False
    if in_sp and n - s0 >= MIN_CH:
        raw.append((max(0, s0-PAD), n))
    if not raw:
        return []

    merged = [list(raw[0])]
    for s, e in raw[1:]:
        if s - merged[-1][1] <= MERGE:
            merged[-1][1] = e
        else:
            merged.append([s, e])

    mx_samp = max_group_sec * SAMPLE_RATE
    groups: list[tuple[int, int]] = []
    gs = merged[0][0] * VAD_CHUNK
    ge = merged[0][1] * VAD_CHUNK
    for seg in merged[1:]:
        s = seg[0] * VAD_CHUNK; e = seg[1] * VAD_CHUNK
        if e - gs > mx_samp:
            groups.append((gs, ge)); gs = s
        ge = e
    groups.append((gs, ge))

    result = []
    for gs, ge in groups:
        ns = max(1, int((ge - gs) // SAMPLE_RATE))
        ch = audio[gs: gs + ns * SAMPLE_RATE].astype(np.float32)
        if len(ch) < SAMPLE_RATE:
            continue
        result.append((gs / SAMPLE_RATE, gs / SAMPLE_RATE + ns, ch))
    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# å­—å¹•æ–·å¥
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ä¸­æ–‡ã€è‹±æ–‡æ¨™é»çµ±ä¸€è§¸ç™¼åˆ‡è¡Œï¼ˆå«è‹±æ–‡é€—è™Ÿï¼‰
_PUNCT = frozenset('ï¼Œã€‚ï¼Ÿï¼ï¼›ï¼šâ€¦â€”ã€.,!?;:')


def split_to_lines(text: str) -> list[str]:
    """ä»¥æ¨™é»ç¬¦è™Ÿåˆ‡åˆ†çŸ­å¥ï¼Œç§»é™¤æ¨™é»ï¼Œæ¯å¥ç¨ç«‹æˆè¡Œã€‚

    æ–·å¥è¦å‰‡ï¼ˆè‹±æ–‡/ä¸­æ–‡çµ±ä¸€ï¼‰ï¼š
    1. æ‰€æœ‰æ¨™é»ï¼ˆ,.!?;: åŠä¸­æ–‡ï¼Œã€‚ï¼Ÿï¼ï¼›ï¼šâ€¦â€”ï¼‰â†’ ç«‹å³åˆ‡è¡Œï¼Œæ¨™é»ä¸è¼¸å‡º
    2. è‹±æ–‡æ•´å­—ç‚ºæœ€å°å–®ä½ï¼Œè©å‰è£œç©ºæ ¼ï¼ˆè©ç•Œï¼‰
    3. MAX_CHARS ä¿è­·ï¼šè¶…é™æ‰å¼·åˆ¶æ›è¡Œ
    """
    if "<asr_text>" in text:
        text = text.split("<asr_text>", 1)[1]
    text = text.strip()
    if not text:
        return []

    lines: list[str] = []
    buf   = ""

    i = 0
    while i < len(text):
        ch = text[i]

        # â”€â”€ æ¨™é»ç¬¦è™Ÿï¼šåˆ‡è¡Œï¼Œæ¨™é»ä¸åŠ å…¥è¼¸å‡ºï¼ˆéš±è—ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if ch in _PUNCT:
            if buf.strip():
                lines.append(buf.strip())
            buf = ""
            i += 1
            continue

        # â”€â”€ è‹±æ–‡å–®å­—ï¼šæ•´å­—æ”¶é›†ï¼Œè©å‰è£œç©ºæ ¼ï¼ˆè©ç•Œï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if ch.isalpha() and ord(ch) < 128:
            j = i
            while j < len(text) and text[j].isalpha() and ord(text[j]) < 128:
                j += 1
            word = text[i:j]
            prefix = " " if buf and not buf.endswith(" ") else ""
            if len(buf) + len(prefix) + len(word) > MAX_CHARS and buf.strip():
                lines.append(buf.strip())
                buf = word
            else:
                buf += prefix + word
            i = j
            continue

        # â”€â”€ ç©ºæ ¼ï¼šä¿ç•™åˆ†è©é–“è· â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if ch == " ":
            if buf and not buf.endswith(" "):
                buf += " "
            i += 1
            if len(buf.rstrip()) >= MAX_CHARS:
                lines.append(buf.strip())
                buf = ""
            continue

        # â”€â”€ ä¸­æ–‡/æ—¥æ–‡/æ•¸å­—ç­‰ï¼šé€å­—ç´¯ç© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        buf += ch
        i += 1
        if len(buf) >= MAX_CHARS:
            lines.append(buf.strip())
            buf = ""

    if buf.strip():
        lines.append(buf.strip())
    return [l for l in lines if l.strip()]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SRT æ™‚é–“è»¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def srt_ts(s: float) -> str:
    """ç§’æ•¸è½‰ SRT æ™‚é–“æˆ³æ ¼å¼ HH:MM:SS,mmm"""
    ms = int(round(s * 1000))
    hh = ms // 3_600_000; ms %= 3_600_000
    mm = ms // 60_000;    ms %= 60_000
    ss = ms // 1_000;     ms %= 1_000
    return f"{hh:02d}:{mm:02d}:{ss:02d},{ms:03d}"


def assign_ts(
    lines: list[str], g0: float, g1: float,
) -> list[tuple[float, float, str]]:
    """ä¾å­—æ•¸æ¯”ä¾‹åˆ†é…æ™‚é–“è»¸ã€‚"""
    if not lines:
        return []
    total = sum(len(l) for l in lines)
    if total == 0:
        return []
    dur = g1 - g0; res = []; cur = g0
    for i, line in enumerate(lines):
        end = cur + max(MIN_SUB_SEC, dur * len(line) / total)
        if i == len(lines) - 1:
            end = max(end, g1)
        res.append((cur, end, line))
        cur = end + GAP_SEC
    return res


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# å³æ™‚è½‰éŒ„ç®¡ç†å“¡
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class RealtimeManager:
    """sounddevice ä¸²æµ + VAD + ç·©è¡è½‰éŒ„ã€‚

    éœ€è¦ ASR å¼•æ“æœ‰ä»¥ä¸‹å±¬æ€§ï¼š
      - vad_sess : ONNX InferenceSessionï¼ˆSilero VADï¼‰
      - transcribe(audio, max_tokens=..., language=..., context=...) -> str
      - max_chunk_secs : intï¼ˆå¯é¸ï¼Œé è¨­ 19ï¼‰

    on_text å›å‘¼ç°½åï¼šon_text(text: str, start_sec: float, end_sec: float)
      start_sec / end_sec ç‚ºç›¸å°æ–¼éŒ„éŸ³é–‹å§‹çš„çœŸå¯¦ç§’æ•¸ã€‚
    """

    def __init__(
        self,
        asr,
        device_idx: int,
        on_text,
        on_status,
        language: str | None = None,
        context: str | None = None,
    ):
        self.asr       = asr
        self.dev_idx   = device_idx
        self.on_text   = on_text    # callback(text: str, start_sec: float, end_sec: float)
        self.on_status = on_status  # callback(msg: str)
        self.language  = language
        self.context   = context
        self._q        = queue.Queue()
        self._running  = False
        self._stream   = None

    def start(self):
        import sounddevice as sd
        self._running = True
        # æŸ¥è©¢è£ç½®åŸç”Ÿè²é“æ•¸ï¼šç«‹é«”è²æ··éŸ³ç­‰ loopback è£ç½®éœ€è¦ 2ch
        dev_info      = sd.query_devices(self.dev_idx, "input")
        self._native_ch = max(1, int(dev_info["max_input_channels"]))
        self._stream  = sd.InputStream(
            device=self.dev_idx,
            samplerate=SAMPLE_RATE,
            channels=self._native_ch,
            blocksize=VAD_CHUNK,
            dtype="float32",
            callback=self._audio_cb,
        )
        threading.Thread(target=self._loop, daemon=True).start()
        self._stream.start()
        self.on_status("ğŸ”´ éŒ„éŸ³ä¸­â€¦")

    def stop(self):
        self._running = False
        if self._stream:
            self._stream.stop()
            self._stream.close()
            self._stream = None
        self.on_status("â¹ å·²åœæ­¢")

    def _audio_cb(self, indata, frames, time_info, status):
        # å¤šè²é“æ··éŸ³å–å¹³å‡è½‰ monoï¼ˆç«‹é«”è²æ··éŸ³ / WASAPI loopback 2chï¼‰
        mono = indata.mean(axis=1) if indata.shape[1] > 1 else indata[:, 0]
        self._q.put(mono.copy())

    def _loop(self):
        h   = np.zeros((2, 1, 64), dtype=np.float32)
        c   = np.zeros((2, 1, 64), dtype=np.float32)
        sr  = np.array(SAMPLE_RATE, dtype=np.int64)
        buf: list[np.ndarray] = []
        sil = 0
        # ç´¯è¨ˆå–æ¨£æ•¸ â†’ çœŸå¯¦æ™‚é–“ï¼ˆæ¯æ”¶åˆ°ä¸€å€‹ chunk +VAD_CHUNKï¼‰
        total_chunks   = 0     # è‡ªéŒ„éŸ³é–‹å§‹çš„ç¸½ chunk æ•¸
        buf_start_chunk = 0    # ç•¶å‰ buf ç¬¬ä¸€å€‹ chunk çš„å…¨åŸŸä½ç½®

        while self._running:
            try:
                chunk = self._q.get(timeout=0.1)
            except queue.Empty:
                continue

            total_chunks += 1

            out, h, c = self.asr.vad_sess.run(
                None,
                {"input": chunk[np.newaxis, :].astype(np.float32), "h": h, "c": c, "sr": sr},
            )
            prob = float(out[0, 0])

            if prob >= VAD_THRESHOLD:
                if not buf:
                    buf_start_chunk = total_chunks - 1
                buf.append(chunk); sil = 0
            elif buf:
                buf.append(chunk); sil += 1
                rt_max_buf = int(getattr(self.asr, "max_chunk_secs", 19) * SAMPLE_RATE / VAD_CHUNK)
                if sil >= RT_SILENCE_CHUNKS or len(buf) >= rt_max_buf:
                    audio = np.concatenate(buf)
                    n = max(1, len(audio) // SAMPLE_RATE) * SAMPLE_RATE
                    # è¨ˆç®—çœŸå¯¦æ™‚é–“è»¸
                    start_sec = buf_start_chunk * VAD_CHUNK / SAMPLE_RATE
                    end_sec   = start_sec + n / SAMPLE_RATE
                    _max_tok = 400 if self.language == "Japanese" else 300
                    try:
                        text = self.asr.transcribe(
                            audio[:n],
                            max_tokens=_max_tok,
                            language=self.language,
                            context=self.context,
                        )
                        if text:
                            self.on_text(text, start_sec, end_sec)
                    except Exception as _e:
                        self.on_status(f"âš  è½‰éŒ„éŒ¯èª¤ï¼š{_e}")
                    buf = []; sil = 0
                    h = np.zeros((2, 1, 64), dtype=np.float32)
                    c = np.zeros((2, 1, 64), dtype=np.float32)
